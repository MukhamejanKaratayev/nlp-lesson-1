{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в NLP\n",
    "\n",
    "1. Введение в NLP\n",
    "2. Установка библиотеки NLTK\n",
    "3. Токенизация текста\n",
    "4. Удаление стоп-слов\n",
    "5. Стемминг и лемматизация\n",
    "6. Разметка частей речи\n",
    "7. Распознавание именованных сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2: Установка NLTK\n",
    "NLTK (Natural Language Toolkit) - популярная библиотека для работы с NLP в Python.\n",
    "Для установки NLTK, вам нужно выполнить команду: pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "\n",
    "# Убедитесь, что вы загрузили необходимые ресурсы NLTK:\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3: Токенизация текста\n",
    "Токенизация - это процесс разделения текста на предложения или слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены слов: ['China', 'blames', 'Canada', 'for', '‘', 'malicious', ',', 'provocative', '’', 'moves', 'after', 'close', 'midair', 'intercepts', 'over', 'South', 'China', 'Sea']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"China blames Canada for ‘malicious, provocative’ moves after close midair intercepts over South China Sea\"\n",
    "word_tokens = word_tokenize(example_text)\n",
    "print(\"Токены слов:\", word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 4: Удаление стоп-слов\n",
    "Стоп-слова - это общие слова, которые часто игнорируются в NLP, так как они мало влияют на смысл текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп-слова: {'between', 'how', 'him', 'themselves', 'out', 'in', 't', 'now', 'be', 'that', 'itself', 'should', 'aren', 'more', 'only', 'yourself', 'than', 'there', 'have', 'and', 'are', 'you', 'it', 'until', 'about', 'here', 'from', 'he', 'shan', \"doesn't\", 'because', \"hadn't\", 'herself', 'which', 'hadn', 'myself', 'all', 'ourselves', \"you've\", 'a', \"shan't\", 'on', \"you'd\", 'm', 'being', 'some', 'been', 'just', 'o', 'hers', 'our', 'such', 're', 'after', 'below', 'as', 'no', 'is', 'through', 'with', 'couldn', 'again', 'down', 's', 'haven', 'i', 'yourselves', 'to', 'up', \"you're\", \"should've\", 'didn', 'its', 'll', 'few', \"shouldn't\", 'weren', \"mustn't\", 'but', 'won', 'these', 'other', \"weren't\", \"it's\", 'of', 'further', 'has', 'd', \"you'll\", 'yours', 'we', 'for', 'nor', \"didn't\", 'doing', 'those', 'both', 'not', 'once', 'don', 'shouldn', 'when', 'very', 'she', 'had', 'own', 'his', \"needn't\", \"won't\", 'my', \"don't\", 'am', 'wasn', 'their', 'at', 'over', 'your', 'will', 'wouldn', 'doesn', 'himself', 'does', 'ain', 'so', 'were', 'did', 'why', 'or', 'do', \"wouldn't\", 'into', 'hasn', 'mightn', \"couldn't\", 'whom', \"that'll\", \"hasn't\", 'who', 'having', 'off', 'her', 'the', 'before', 'they', 'this', 'y', \"aren't\", 'can', 'same', 've', 'then', 'against', 'ma', 'too', 'under', 'was', \"isn't\", 'any', \"she's\", 'them', 'during', 'mustn', 'ours', 'each', 'what', \"mightn't\", 'if', 'most', \"haven't\", 'where', 'above', 'isn', 'needn', 'by', 'me', \"wasn't\", 'an', 'theirs', 'while'}\n",
      "Отфильтрованные слова: ['China', 'blames', 'Canada', '‘', 'malicious', ',', 'provocative', '’', 'moves', 'close', 'midair', 'intercepts', 'South', 'China', 'Sea']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print('Стоп-слова:', stop_words)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(\"Отфильтрованные слова:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 5: Стемминг и лемматизация\n",
    "Стемминг - это процесс приведения слов к их корневой форме.\n",
    "Лемматизация - это более сложный процесс, который учитывает контекст и приводит слово к его базовой форме.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['China', 'blames', 'Canada', '‘', 'malicious', ',', 'provocative', '’', 'moves', 'close', 'midair', 'intercepts', 'South', 'China', 'Sea']\n",
      "Стеммированные слова: ['china', 'blame', 'canada', '‘', 'malici', ',', 'provoc', '’', 'move', 'close', 'midair', 'intercept', 'south', 'china', 'sea']\n",
      "Лемматизированные слова: ['China', 'blame', 'Canada', '‘', 'malicious', ',', 'provocative', '’', 'move', 'close', 'midair', 'intercept', 'South', 'China', 'Sea']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_words = [stemmer.stem(w) for w in filtered_sentence]\n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_sentence]\n",
    "print('Original: ', filtered_sentence)\n",
    "print(\"Стеммированные слова:\", stemmed_words)\n",
    "print(\"Лемматизированные слова:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 6: Разметка частей речи\n",
    "POS-tagging (Part-Of-Speech tagging) - это процесс определения части речи для каждого слова.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('China', 'NNP')\n",
      "('blames', 'VBZ')\n",
      "('Canada', 'NNP')\n",
      "('‘', 'NNP')\n",
      "('malicious', 'JJ')\n",
      "(',', ',')\n",
      "('provocative', 'JJ')\n",
      "('’', 'NN')\n",
      "('moves', 'NNS')\n",
      "('close', 'RB')\n",
      "('midair', 'JJ')\n",
      "('intercepts', 'NNS')\n",
      "('South', 'NNP')\n",
      "('China', 'NNP')\n",
      "('Sea', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(filtered_sentence)\n",
    "for tag in pos_tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке NLTK для разметки частей речи (POS-tagging) используется набор тегов, основанный на корпусе Penn Treebank. Вот некоторые из наиболее часто встречающихся тегов и их значения:\n",
    "\n",
    "\n",
    "* NN: Существительное в единственном числе (noun, singular or mass)\n",
    "* NNS: Существительное во множественном числе (noun plural)\n",
    "* NNP: Имя собственное в единственном числе (proper noun, singular)\n",
    "* NNPS: Имя собственное во множественном числе (proper noun, plural)\n",
    "* PRP: Личное местоимение (personal pronoun)\n",
    "* PRP$: Притяжательное местоимение (possessive pronoun)\n",
    "* RB: Наречие (adverb)\n",
    "* RBR: Сравнительная степень наречия (adverb, comparative)\n",
    "* RBS: Превосходная степень наречия (adverb, superlative)\n",
    "* VB: Глагол в основной форме (verb, base form)\n",
    "* VBD: Глагол в прошедшем времени (verb, past tense)\n",
    "* VBG: Глагол или существительное в форме настоящего длительного времени (verb, gerund/present participle)\n",
    "* VBN: Прошедшее причастие (verb, past participle)\n",
    "* VBP: Глагол в настоящем времени, не 3-е лицо единственного числа (verb, non-3rd person singular present)\n",
    "* VBZ: Глагол в настоящем времени, 3-е лицо единственного числа (verb, 3rd person singular present)\n",
    "* JJ: Прилагательное (adjective)\n",
    "* JJR: Сравнительная степень прилагательного (adjective, comparative)\n",
    "* JJS: Превосходная степень прилагательного (adjective, superlative)\n",
    "* DT: Артикль (determiner)\n",
    "* IN: Предлог или подчинительный союз (preposition/subordinating conjunction)\n",
    "* CC: Союз (coordinating conjunction)\n",
    "* MD: Модальный глагол (modal)\n",
    "* TO: частица \"to\" (to)\n",
    "* WDT: Определяющее слово (wh-determiner)\n",
    "* WP: Местоимение вопросительное (wh-pronoun)\n",
    "* WRB: Наречие вопросительное (wh-adverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 7: Распознавание именованных сущностей\n",
    "Распознавание именованных сущностей (Named Entity Recognition, NER) - это процесс определения и классификации именованных сущностей в тексте в категории, такие как имена, места, компании, даты, суммы денег и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Именованные сущности и категории:\n",
      "PERSON Mark\n",
      "PERSON John\n",
      "ORGANIZATION Google\n"
     ]
    }
   ],
   "source": [
    "# Пример текста для NER\n",
    "ner_text = \"Mark and John are working at Google since 2010\"\n",
    "ner_tokens = word_tokenize(ner_text)\n",
    "\n",
    "# Применяем POS-tagging перед NER\n",
    "ner_tags = pos_tag(ner_tokens)\n",
    "\n",
    "# Используем ne_chunk для распознавания именованных сущностей\n",
    "named_entities = ne_chunk(ner_tags)\n",
    "\n",
    "# ne_chunk возвращает дерево с именованными сущностями и категориями\n",
    "print(\"Именованные сущности и категории:\")\n",
    "for entity in named_entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(entity.label(), ' '.join(c[0] for c in entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот скрипт демонстрирует базовое использование NER для определения людей и организаций в тексте.\n",
    "\n",
    "В заключение, давайте обсудим, как эти инструменты могут быть использованы в реальных задачах NLP.\n",
    "Например, токенизация и удаление стоп-слов полезны для предобработки данных перед их использованием в моделях машинного обучения.\n",
    "Стемминг и лемматизация помогают уменьшить размерность векторного пространства приложений, таких как моделирование тем или поиск по тексту.\n",
    "POS-tagging и NER используются для извлечения информации и понимания структуры предложений для более сложных задач, таких как автоматическое резюмирование текста или вопросно-ответные системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Obtaining dependency information for natasha from https://files.pythonhosted.org/packages/32/9c/bb9d33c13564bcc939bb727087ef51b16ed3b49cc3b8fdec07c87b02f1de/natasha-1.6.0-py3-none-any.whl.metadata\n",
      "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pymorphy2 (from natasha)\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m355.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hCollecting razdel>=0.5.0 (from natasha)\n",
      "  Using cached razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting navec>=0.9.0 (from natasha)\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting slovnet>=0.6.0 (from natasha)\n",
      "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m51.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
      "  Obtaining dependency information for yargy>=0.16.0 from https://files.pythonhosted.org/packages/b7/55/d065a9812c619889fbe01a1863743ee45f7c60c462fc95b19576972ee9e4/yargy-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting ipymarkup>=0.8.0 (from natasha)\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/mukhamejankaratyev/anaconda3/envs/churn_prediction_env/lib/python3.10/site-packages (from navec>=0.9.0->natasha) (1.25.1)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m23.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2->natasha)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree>=3->ipymarkup>=0.8.0->natasha)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m37.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:22\u001b[0m\n",
      "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: intervaltree\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=c30bd26d04a7776db37fdb9ea75c671ffddfec25728693dc0c7e294e9610ef98\n",
      "  Stored in directory: /Users/mukhamejankaratyev/Library/Caches/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
      "Successfully built intervaltree\n",
      "Installing collected packages: sortedcontainers, razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 sortedcontainers-2.4.0 yargy-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
    "doc = Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(text='Посол Израиля на Украине Йоэль Лион признался, чт...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=5, text='Посол'), DocToken(start=6, stop=13, text='Израиля'), DocToken(start=14, stop=16, text='на'), DocToken(start=17, stop=24, text='Украине'), DocToken(start=25, stop=30, text='Йоэль')]\n"
     ]
    }
   ],
   "source": [
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=5, text='Посол', pos='NOUN', feats=<Anim,Nom,Masc,Sing>, lemma='посол'), DocToken(start=6, stop=13, text='Израиля', pos='PROPN', feats=<Inan,Gen,Masc,Sing>, lemma='израиль'), DocToken(start=14, stop=16, text='на', pos='ADP', lemma='на'), DocToken(start=17, stop=24, text='Украине', pos='PROPN', feats=<Inan,Loc,Fem,Sing>, lemma='украина'), DocToken(start=25, stop=30, text='Йоэль', pos='PROPN', feats=<Anim,Nom,Masc,Sing>, lemma='йоэль')]\n"
     ]
    }
   ],
   "source": [
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_prediction_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
